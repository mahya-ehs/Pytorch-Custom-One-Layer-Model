# -*- coding: utf-8 -*-
"""oneLayerModel-version2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1le_7XoRC44z7jmrElPzAdHTv8wYJh9c7

**import libraries**
"""

import torch
import torch.nn as nn
import torch.optim as optim
import math
from torchvision import datasets, transforms
from torch.autograd import Function
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
import sklearn.cluster

use_gpu = True
device = torch.device("cuda:0" if torch.cuda.is_available() and use_gpu else "cpu")
device

"""**custom linear layer**"""

class LinearFunction(Function):
    @staticmethod
    def forward(ctx, input, weight, bias=None):
        ctx.save_for_backward(input, weight, bias)
        output = input.mm(weight.t()) + bias
        return output

    @staticmethod
    def backward(ctx, grad_output):
        #grad_output -> dLoss/dy_hat
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias

class MyLinearLayer(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size

        self.weight = nn.Parameter(torch.randn(output_size, input_size))
        self.bias = nn.Parameter(torch.randn(output_size))

    def forward(self, input):
        return LinearFunction.apply(input, self.weight, self.bias)

"""**MSE Loss**"""

class MSELossFunction(Function):
    @staticmethod
    def forward(ctx, y_pred, y):
        y = y.view(y.shape[0], -1)
        ctx.save_for_backward(y_pred, y)
        loss = ( (y - y_pred)**2 ).mean()

        return  loss

    @staticmethod
    def backward(ctx, grad_output):
        y_pred, y = ctx.saved_tensors
        grad_input = 2 * (y_pred - y) / y_pred.shape[0]
        return grad_input, None

class MSELoss(nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, input, target):
        return MSELossFunction.apply(input, target)

"""**custom cross entropy loss**"""

class CrossEntropyLossFunction(Function):
    @staticmethod
    def forward(ctx, output, target):
        output_softmax = F.log_softmax(output, dim=1)

        one_hot_labels = torch.zeros_like(output_softmax)
        one_hot_labels.scatter_(1, target.view(-1, 1), 1)

        ctx.save_for_backward(output_softmax, one_hot_labels)

        loss = torch.sum(-one_hot_labels * output_softmax, dim=1).mean()

        return loss

    @staticmethod
    def backward(ctx, grad_output):
        output, target = ctx.saved_tensors

        grad_input = (F.softmax(output, dim=1) - target)/output.shape[0]

        return grad_input, None

class CrossEntropyLoss(nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, input, target):
        return CrossEntropyLossFunction.apply(input, target)

"""**model with one layer**"""

class BasicModel(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.layer = nn.Linear(input_size, output_size)

    def forward(self, x):
        x = x.view(-1, 784)
        return self.layer(x)

"""**dataLoader**"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

"""**model and loss function**"""

input_size = 784
output_size = 10

model = BasicModel(input_size, output_size)
criterion = CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

model = model.to(device)

"""**train function**"""

def train(model, dataloader, loss_fn, optimizer, num_epochs=50):
    for epoch in range(num_epochs):
        model.train()
        num_samples = len(dataloader.dataset)
        num_batches = len(dataloader)
        running_corrects = 0
        running_loss = 0.0
        for index, (inputs, labels) in enumerate(dataloader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            _, preds = torch.max(outputs, dim=1)
            running_corrects += torch.sum(preds == labels)
            running_loss += loss.item()

        epoch_loss = (running_loss / num_batches)
        epoch_acc = (running_corrects / num_samples) * 100
        print(f"epoch {epoch+1} -> Loss: {epoch_loss}, accuracy: {epoch_acc}")

train(model, train_loader, criterion, optimizer)

"""**test function**"""

def test(model, dataloader):
    model.eval()
    correct = 0
    total_correct = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs.data, 1)
            total_correct += labels.size(0)
            correct += (preds == labels).sum().item()

    print(f"Accuracy: {(correct / total_correct) * 100}%")

test(model, test_loader)